[libevent]:
libevent是基于Reactor模型实现的一种IO框架库, 默认是单线程+IO多路复用, 
因此在事件处理器中尽量不要绑定耗时操作的回调函数, 可以将其放到新线程中或组织在线程池中
1. libevent统一了事件源, 可以对IO事件, 信号事件, 定时事件进行一个统一的管理.
2. 采用事件通知的形式来管理事件, 实现了高性能.
3. libevent也可以实现多线程, 在每个线程中创建一个reactor实例.
主线程负责监听socket连接, 完成一些accept操作, 将就绪的连接分发到worker线程中.
在worker线程中负责监听IO读写事件. 这个分发的过程需要同步, 保证主线程的分发与worker线程的获取是线程安全的.

[libevent的流程]:
1. 创建event_base并初始化, 相当于创建了一个reactor实例.
2. 初始化事件, 对事件源, 事件类型, 事件处理函数等进行一个初始化
3. 指明事件从属的reactor实例, 如果说程序中创建了多个reactor实例的话必须指明
4. 注册事件, 将对应事件添加到对应的注册数据结构中. 比如说IO与信号事件被组织在双向循环链表中, 
定时事件组织在小根堆中.
5. 监听事件, 对事件进行一个循环的监听. 如果底层使用的是epoll, 那就是调用的epoll_wait()
当事件就绪时, 将就绪的事件添加到就绪链表中, 然后对就绪链表中所有的就绪事件调用其对应的事件处理函数, 进行处理.

[如何统一的事件源]:
1. 统一事件源, 其实就是将信号事件, 定时事件能像IO事件一样, 像epoll, select一样对事件进行一个监听, 就绪后进行一个处理.
2. 定时事件, 定时事件没有对应的像信号, 描述符这种实体的事件源, 仅仅是一个超时时间.
它其实是利用了底层epoll_wait或select监听函数中的超时时间参数进行了一个对照. 
将epoll_wait或select中监听的超时时间设置成小根堆中最小的超时时间, 当监听超时时, 说明最早超时的定时事件就绪了,
那么将该就绪事件放到就绪链表中进行一个处理.
3. 信号事件, 信号事件如果发生会自动调用信号处理函数, 无法对epoll_wait进行一个通知.
底层是通过一个socket pair来解决的, 创建2个socket, 一个读socket, 一个写socket(地址通过本地环回地址)
将读socket注册到事件多路分发器中, 当信号到来时, 通过写socket往连接中写1个字节的数据,
此时socket读事件就绪, 将该信号事件放到就绪链表中进行处理.

[event_base框架中的eventop结构体]:
1. 其实就是对事件多路分发器的一个描述和组织, 里面主要包含了事件的注册, 移除, 监听等回调函数
2. 通过修改宏可以指定底层的IO复用, 在event_base初始化时, 会将eventop中的回调函数指向对应IO复用的一些函数.
[event]:
1. event结构体是对事件的一个描述和组织
2. 比如说事件源, 事件的类型, 事件处理函数, 事件的状态等信息


[muduo网络库]:
muduo是多线程+one loop per thread, 即所谓的主从模式, 并且每个线程中有一个事件多路分发器.
主线程完成一些连接操作, 比如说accept, 其他线程主要负责一些worker, 比如说read, write以及
其他的数据处理等操作

[Reactor与Proactor, 高效的事件驱动模型]
1. Reactor是基于同步实现的, Proactor是基于异步实现的
2. Reactor中, 主线程只负责监听文件描述符上是否有事件发生, 如果有则通知
该事件的工作线程. 主线程不做任何实质性的工作, IO操作, 逻辑处理都交给工作线程.
Proactor中, IO操作交给主线程和内核, 逻辑操作交给工作线程

[高并发模型]:
1. 半同步半异步. 典型的muduo网络库就是基于半同步半异步的一种高并发模型.
采用one loop per thread, 支持多线程并且每个线程都有一个事件循环器.
主线程主要负责监听描述符是否连接, 完成一些accept操作, 当描述符就绪时, 将连接描述符通过管道交给worker线程.
worker线程主要负责监听连接描述符上的读写事件, 完成一些IO读写操作.
对于一些cpu密集型的耗时操作可以放到muduo提供的线程池中来进行一个管理.
2. 领导者与追随者. 
线程池中的线程总共有三种状态, leader, processer, follower.
leader线程负责监听IO事件, 当有事件就绪时, 可以选择对该事件进行处理并选出新的leader,
或者将该就绪事件交给其他的follower来进行处理, 其leader的身份不变;
processer线程是正在处理IO事件的线程.
follower是睡眠在线程池中的线程, 准备当选新的领导者或对就绪的IO事件进行处理.
(不能让每个线程独立的去管理多个描述符连接)


[zookeeper]:
zookeeper是开源的分布式框架, 主要是为了解决分布式数据管理的问题
1. zookeeper主要包含2个部分, 分别是文件系统和通知机制,
zookeeper底层是通过树形结构来组织数据节点, 之所以通过树形结构是为了更好的组织不同服务
的层次关系, 每个数据节点能存储1M的数据大小.
通过观察者模式来监听数据节点, zookeeper在启动时会产生2个线程, 一个是connect线程主要负责
连接操作以及将客户端关心的数据放到监听列表中. 另一个是一个listener线程, 主要负责监听
列表中数据的变化, 当数据变化时会通知listener线程去调用process方法去处理.
2. 在我的项目中, 将zookeeper作为服务注册中心, 服务端在启动时将自己的服务注册到
zookeeper上, 客户端在进行rpc调用时会查看zookeeper的服务列表, 返回服务的ip与port,
完成rpc调用请求的发送
在这个过程中, 客户端不需要记住服务的ip和port, 服务是否注册, 是否启动以及服务的ip和port
是什么都交给zookeeper进行管理

[zookeeper集群管理, ZAB协议]:
1. zookeeper为了解决数据的一致性问题, 它主要包含2个方面, 一个是消息广播, 一个是崩溃恢复.
2. 消息广播是指, 当客户端发送write请求时, 需要通过leader服务器来进行数据的广播.
保证数据一致性. 如果说follower接收到了write请求, 需要转发给leader. 
  1) leader将write封装成事物, 写入到事物日志后, 将该事物广播给其他的follower.
  follower收到事物后, 写入到事物日志中, 进行一个ack回复.
  2) 当leader收到的ack满足过半机制后, 将事物日志中的最新事物进行一个commit.
  然后同步给其他的folloer, 也进行一个commit.
  (2阶段提交)
3. 崩溃恢复是指, 当leader崩溃后, 需要通过重新选举leader来保证zookeeper集群的正常运行.
选举的规则, 为了尽可能的保证数据的恢复, 优先选择zxid(事物id)最大的作为leader.
如果说zxid都相同, 再按照myid大的作为leader.这样一个选举规则
然后leader再协调进行一个数据的同步或回滚, 来保证数据的一致性



